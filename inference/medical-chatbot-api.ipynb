{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#05988A\" id='Install-Libraries'> Install Libraries </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#05988A\"> Required</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-22T10:50:30.280689Z",
     "iopub.status.busy": "2025-08-22T10:50:30.279867Z",
     "iopub.status.idle": "2025-08-22T10:52:26.690164Z",
     "shell.execute_reply": "2025-08-22T10:52:26.689095Z",
     "shell.execute_reply.started": "2025-08-22T10:50:30.280650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "\n",
    "!pip install fastapi nest-asyncio pyngrok uvicorn \n",
    "!pip install groq -q\n",
    "\n",
    "!pip install pip3-autoremove\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install unsloth\n",
    "!pip install --upgrade transformers==4.52.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#05988A\" id='FastAPI-App-Skeleton'> FastAPI App Skeleton </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T10:52:26.692233Z",
     "iopub.status.busy": "2025-08-22T10:52:26.691920Z",
     "iopub.status.idle": "2025-08-22T10:52:28.155750Z",
     "shell.execute_reply": "2025-08-22T10:52:28.155014Z",
     "shell.execute_reply.started": "2025-08-22T10:52:26.692199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from fastapi import FastAPI, Depends, HTTPException\n",
    "import os\n",
    "from groq import Groq\n",
    "import re\n",
    "from typing import List, Optional, Dict\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Loading GROQ API Key\n",
    "user_secrets = UserSecretsClient()\n",
    "groq_api_key = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# middlewares\n",
    "app.add_middleware(\n",
    "    CORSMiddleware, # https://fastapi.tiangolo.com/tutorial/cors/\n",
    "    allow_origins=['*'], # wildcard to allow all, more here - https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Access-Control-Allow-Origin\n",
    "    allow_credentials=True, # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Access-Control-Allow-Credentials\n",
    "    allow_methods=['*'], # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Access-Control-Allow-Methods\n",
    "    allow_headers=['*'], # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Access-Control-Allow-Headers\n",
    ")\n",
    "\n",
    "groq = Groq(api_key = groq_api_key)\n",
    "\n",
    "def language_identify(chat_msg):\n",
    "    \n",
    "    prompt = f'''Identify the chat message into one of these languages: \n",
    "    (1) English, (2) Bengali.\n",
    "    If you can't figure out a category, use \"UnIdentified\".\n",
    "    Put the category inside <category> </category> tags. \n",
    "    Chat message: {chat_msg}'''\n",
    "\n",
    "    chat_completion = groq.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        # model=\"llama-3.3-70b-versatile\",\n",
    "        model=\"deepseek-r1-distill-llama-70b\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    content = chat_completion.choices[0].message.content\n",
    "    match = re.search(r'<category>(.*)<\\/category>', content, flags=re.DOTALL)\n",
    "    category = \"UnIdentified\"\n",
    "    if match:\n",
    "        category = match.group(1)\n",
    "\n",
    "    return category\n",
    "\n",
    "\n",
    "def filter_unique_bengali_sentences_and_rejoin(paragraph):\n",
    "    \"\"\"\n",
    "    Filters unique sentences from a Bengali paragraph and rejoins them.\n",
    "\n",
    "    Args:\n",
    "        paragraph (str): The Bengali paragraph as a string.\n",
    "\n",
    "    Returns:\n",
    "        str: A new paragraph containing only the unique sentences.\n",
    "    \"\"\"\n",
    "    # Replace common sentence-ending punctuations with a single delimiter for splitting\n",
    "    paragraph = re.sub(r'([?.!])', r'\\1ред', paragraph)\n",
    "    \n",
    "    # Split the paragraph into a list of sentences using 'ред' as the delimiter\n",
    "    sentences = [s.strip() for s in paragraph.split('ред') if s.strip()]\n",
    "    \n",
    "    # Use a set to store unique sentences while preserving order\n",
    "    # This is a common pattern to get unique items in a list while maintaining order.\n",
    "    # The 'dict.fromkeys' method is an efficient and Pythonic way to do this.\n",
    "    unique_sentences_ordered = list(dict.fromkeys(sentences))\n",
    "    \n",
    "    # Join the unique sentences back into a single paragraph\n",
    "    # The 'ред ' is used to ensure sentences are separated by a full stop and a space.\n",
    "    rejoined_paragraph = 'ред '.join(unique_sentences_ordered)\n",
    "    \n",
    "    return rejoined_paragraph + 'ред' if rejoined_paragraph else ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#05988A\" id='ngrok API setup'> ngrok API setup </h1>\n",
    "\n",
    "#### We will use `ngrok` to run our fastAPI app temporarily\n",
    "\n",
    "#### [https://ngrok.com](https://ngrok.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T10:52:28.156850Z",
     "iopub.status.busy": "2025-08-22T10:52:28.156587Z",
     "iopub.status.idle": "2025-08-22T10:52:29.293324Z",
     "shell.execute_reply": "2025-08-22T10:52:29.292710Z",
     "shell.execute_reply.started": "2025-08-22T10:52:28.156833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n",
      "Public URL: https://3825d77f03d3.ngrok-free.app\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "\n",
    "# specify a port\n",
    "port = 8004\n",
    "!ngrok config add-authtoken 315IFEYCwihgPaWLvk51lTX79tK_6HUBEHQxeNKmvpyuYvRFK\n",
    "ngrok_tunnel = ngrok.connect(port)\n",
    "\n",
    "# where we can visit our fastAPI app\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#05988A\" id=\"LLM-App\"> LLM App </h1>\n",
    "\n",
    "#### Now let's run a LLM fastAPI app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T10:52:29.295538Z",
     "iopub.status.busy": "2025-08-22T10:52:29.295307Z",
     "iopub.status.idle": "2025-08-22T10:53:06.184840Z",
     "shell.execute_reply": "2025-08-22T10:53:06.184255Z",
     "shell.execute_reply.started": "2025-08-22T10:52:29.295517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯже Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 10:52:40.247017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755859960.452208      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755859960.510003      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯже Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T10:53:06.185677Z",
     "iopub.status.busy": "2025-08-22T10:53:06.185496Z",
     "iopub.status.idle": "2025-08-22T10:53:06.194448Z",
     "shell.execute_reply": "2025-08-22T10:53:06.193757Z",
     "shell.execute_reply.started": "2025-08-22T10:53:06.185662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Request/Response models\n",
    "\n",
    "class MedicalQuery(BaseModel):\n",
    "    question: str = Field(..., \n",
    "                         description=\"Medical question to ask\")\n",
    "\n",
    "class MedicalResponse(BaseModel):\n",
    "    response: str\n",
    "    \n",
    "\n",
    "class QnaModel:\n",
    "    \"\"\"\n",
    "    QnaModel: Fine-Tuned LLAMA Model\n",
    "    \"\"\"\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the model\"\"\"\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"samanta-scratch/bilingual-health-qna\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "\n",
    "        self.tokenizer = get_chat_template(\n",
    "        self.tokenizer,\n",
    "        chat_template = \"llama-3.1\",\n",
    "        )\n",
    "        FastLanguageModel.for_inference(self.model) # Enable native 2x faster inference\n",
    "\n",
    "    \n",
    "    async def infer(self, input_data: MedicalQuery) -> MedicalResponse:  # dependency\n",
    "        \"\"\"Runs a prediction\"\"\"\n",
    "        if not self.model:\n",
    "            # raise RuntimeError(\"Model is not loaded\")\n",
    "            raise HTTPException(status_code=400, detail=\"Model is not loaded\")\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"{input_data.question}\"},\n",
    "        ]\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "\n",
    "        lang = language_identify(input_data.question)\n",
    "        if \"english\" in lang.lower():\n",
    "            outputs = self.model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
    "                                temperature = 0.1, min_p = 0.1)\n",
    "            text = self.tokenizer.batch_decode(outputs)\n",
    "            response_text = (text[0].split('<|end_header_id|>')[-1]).split('<|eot_id|>')[0]\n",
    "        else:\n",
    "            outputs = self.model.generate(input_ids = inputs, max_new_tokens = 256, use_cache = True,\n",
    "                                temperature = 0.1, min_p = 0.1)\n",
    "            text = self.tokenizer.batch_decode(outputs)\n",
    "            text = (text[0].split('<|end_header_id|>')[-1]).split('<|eot_id|>')[0]\n",
    "            response_text = filter_unique_bengali_sentences_and_rejoin(text)\n",
    "\n",
    "        \n",
    "        print(response_text)\n",
    "\n",
    "        return MedicalResponse(response=response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T10:53:06.195260Z",
     "iopub.status.busy": "2025-08-22T10:53:06.195068Z",
     "iopub.status.idle": "2025-08-22T10:53:06.212931Z",
     "shell.execute_reply": "2025-08-22T10:53:06.212209Z",
     "shell.execute_reply.started": "2025-08-22T10:53:06.195244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/2229455109.py:35: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n"
     ]
    }
   ],
   "source": [
    "qna_model = QnaModel()\n",
    "\n",
    "\n",
    "@app.get('/')\n",
    "def index():\n",
    "    return {\n",
    "    \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explain the importance of staying hydrated and its benefits for overall health.\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# post request that takes a review (text type) and returns a sentiment score\n",
    "@app.post('/infer')\n",
    "async def infer(output: MedicalResponse = Depends(qna_model.infer, )) -> MedicalResponse:\n",
    "    \"\"\"\n",
    "    example post request body:\n",
    "    \n",
    "    {\n",
    "        \"question\": \"ржнрж┐рж╢ржи ржерзЗрж░рж╛ржкрж┐ ржХрж┐?\"\n",
    "    }\n",
    "    \n",
    "    example response:\n",
    "    {\n",
    "    \"response\": \"ржнрж┐рж╢ржи ржерзЗрж░рж╛ржкрж┐ рж╣рж▓ ржПржХржЯрж┐ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ ржпрж╛ ржЖржкржирж╛рж░ ржжрзГрж╖рзНржЯрж┐ ржПржмржВ ржжрзГрж╖рзНржЯрж┐ржнржЩрзНржЧрж┐ ржЙржирзНржиржд ржХрж░рждрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рзЗред ржПржЯрж┐ ржкрзНрж░рж╛ржпрж╝рж╢ржЗ ржПржХржЬржи ржЪрж┐ржХрж┐рзОрж╕ржХ ржмрж╛ ржПржХржЬржи ржкрзЗрж╢рж╛ржжрж╛рж░ ржжрзГрж╖рзНржЯрж┐ржнржЩрзНржЧрж┐ ржкрж░рж┐ржЪрж╛рж▓ржХ ржжрзНржмрж╛рж░рж╛ ржкрж░рж┐ржЪрж╛рж▓рж┐ржд рж╣ржпрж╝\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    return output\n",
    "\n",
    "\n",
    "# load the model asynchronously on startup\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    qna_model.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#05988A\" id=\"Run-API\"> Run API </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally run the app\n",
    "uvicorn.run(app, port=port)\n",
    "\n",
    "# it will take some moment (~10 sec) to load the model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
